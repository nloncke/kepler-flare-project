\documentclass[11pt]{article}
\usepackage[margin=1.3in]{geometry}
% \usepackage{showframe}
\usepackage{caption}
\usepackage{float}
\usepackage{enumerate}
\usepackage{array}

\author{Nicole Loncke \& Lucianne Walkowicz}
\date{\today}
\title{Interesting Title...Stellar Flares in Kepler}

\begin{document}

\maketitle{}

\section{Introduction}
\label{sec:intro}

The Kepler Mission is designed to survey our galaxy in the hopes of
discovering planets in or near the habitable zone of their stars and
determine how many of the billions of stars in our galaxy have such
planets. It essentially stares at a relatively small portion of the
sky for a long time to gather brightness data about these stars.

In addition to planet detection, this data can be used to gather
properties about the stars themselves.  In this paper we are concerned
with the flaring behaviors of these nearby stars and their effect on
any orbiting planets.


\section{Using the Tools}
\label{sec:tools}

In this section I describe the tools I've written to facilitate the
vetting process.

\subsection{Formatting}
\label{sec:format}

The \verb|lightcurves| module makes some assumptions about the format
of the input data.  The original light curve data should be in a file
containing a whitespace-separated table with time in the first column
and flux in the second.
\begin{table}[h]
  \centering
  \begin{tabular}{>{\itshape}p{0.2\linewidth} >{\itshape}l}
       808.51470   &   6338.22 \\
       808.53514   &   6340.73 \\
       808.55557   &   6346.89 \\
       808.57601   &   6341.10 \\
       808.59644   &   6340.22 \\
  \end{tabular}
  \caption{Light curve data sampled from Kepler ID 10068383.}
\end{table}

The other fundamental input file is the one containing the potential
flare events, or ``flags.''  This file contains one column listing
indices into the time array at the points that mark a suspected event.

% \begin{table}[h]
%   \centering
%   \begin{tabular}{>{\itshape}p{0.2\linewidth}l}
%          350 \\
%          351 \\
%          352 \\
%          370 \\
%          371 \\
%          372 \\
%          373 \\
%          374 \\
%          375 \\
% \end{tabular}
% \caption{Flags sampled from the file corresponding to Kepler ID 10068383.}
% \end{table}

Those two files --- the light curve data and the flags --- are all you
need to start using these tools.


\subsection{Plotting}
\label{sec:basic}

If your aim is simply to generate arrays of the light curve data ---
one array for time, another for mean-normalized flux --- then use
\verb|ltcurve()|.  This function takes as its primary argument a
string of the name of the file containing the Kepler data and returns
the time and brightness arrays.  By default it also displays the light
curve corresponding to the file on a time vs. brightness plot, but
this feature can be switched off by passing the function an optional
argument.

If you have \emph{multiple} light curve files and would like to view
them one at a time, then \verb|ltcurves()| is more appropriate.  Its
only required argument is a list or array of filename strings.  Note
that this function does not return any of the data.  In addition, if
for each of the Kepler data files you have a set of corresponding
event flags\footnote{You can generate these flags using
  \texttt{getflags()} and passing it a list of the names of the files
  holding the flare flags.}, you may use the \verb|flags| kwarg to
overplot the potential flares.


\subsection{Vetting}
\label{sec:vet}

Instead of cycling through the light curves with overplotted flags,
you may find it helpful to inspect and record whether or not the
marked events could potentially be stellar flares.  In that case, you
should use \verb|flareshow()|, which writes user input (either 'y',
'n', 'm') to two files for later retrieval.  One file contains a
space-separated table of the Kepler IDs and the corresponding user
responses to its events.  The other file contains information about
the length of each event.  These two files work in conjunction to
gather more information about the potential flares.

\begin{table}[h]
  \centering
  \begin{tabular}{l l}

8848271 &  n \\
8908102 &  n \\
8953257 &  n  n  n  n  n  n  n  n \\
9002237 &  n  n  n  y \\
\end{tabular}
\caption{Example output.txt file.}
\end{table}

\begin{table}[h]
  \centering
  \begin{tabular}{l l}

8848271 &  3735 03 \\
8908102 &  1757 03 \\
8953257 &  1454  6 1610  7 1890  4 2359  3 2516  4 2829  5 2985  6
3265  5 \\
9002237 &  3337  4 3547  5 3756  3 3967  4 \\
\end{tabular}
\caption{Corresponding example output\_indices.txt file.}
\end{table}

Note that before using \verb|flareshow()|, you must have your flags in
the proper format, generated by \verb|getflags()|.  This helper
function outputs a nested list of event indices given a list of the
names of the files containing the flags.

\section{Data Processing}
\label{sec:advanced}

After evaluating the marked events by eye, quantifying data about the
remaining candidates is the next step.  Assuming that you used
\verb|flareshow()| for vetting, you now have two output files for the
set of light curve data.  Use the helper function \verb|getEvents()|,
which reads from these output files to pare down your list of flags to
only those that have been marked with 'y' or 'm' depending on how you
set the kwargs.

To calculate the cumulative brightness of each event found within a
single light curve, use \verb|intFlare()|.  This function returns an
array of the integrated brightness over the course of the events, an
array of the duration of the events (in hours), and the peak
brightnesses of each event.  It is important to note that this
function assumes you have vetted the flags already and are only
providing those about which you would like to find more information
(ie, you have used \verb|getEvents()|).

% sections: intro (what even is kepler), general progress to date:
% vetting code, yay human eye but boo to slowness, why not machine
% learning?  relevant features are captured in flare_feature_calc and
% then sklearn runs with it: Support Vector Classifier, linear kernel

% generate some classifier reports and include them in the pdf
% try random forest, some other classifiers from LW link
\section{Machine Learning}
\label{sec:ml}

After creating the tools for by-eye vetting, the major bottleneck for
data analysis was just that---as a human, vetting manually takes a lot
of time.  The flare detection program that produces the flare flags is
trained to recognize some data metrics but does not correctly identify
events with high accuracy.  Our human brains allow us to do the same
thing but with more nuance.  If we could write a program to recognize
the same patterns that humans so easily detect in the light curves then
we could nearly entirely automate the vetting process.  To accomplish
this goal, we trained a classifier with a handful of metrics from each
potential flare event and their respective light curves.

\subsection{Training}
\label{sec:train}
 Our first task was to gather quantitative data about the
stellar flares to feed into the classifier.  In total we use 10
metrics.
\begin{enumerate}[(a)]
\item \emph{amplitude}: the range of the entire light curve.  Stars
  with great stellar variability tend to be more magnetically active
  than those without.  We expect high light curve amplitude to
  correlate with real flares.
\item \emph{number of events}: Light curves that have many flagged
  events tend to have real flares, so we expect a high number of
  events to correlate with real flares.
\item \emph{standard deviation}: The standard deviation of the entire
  light curve with stellar variability subtracted.  \textbf{Though I'm
    not exactly sure of the direct bearing of this metric on the
    likelihood of the event, it seemed like it could be a useful
    metric.}
\item \emph{consecutive points}: Sometimes there are gaps in the
  Kepler data.  Kepler must rotate and point its antenna towards Earth
  to send its light curve data roughly every month.  When the
  satellite begins recording again, there may be a sudden increase in
  brightness that resembles a flare but isn't.  In order to avoid
  marking these as true flares we check whether the time intervals are
  evenly spaced across the event.
\item \emph{kurtosis}: The kurtosis measures the ``peakedness'' of a
  flare event.  A sharp increase and decrease in brightness is likely
  to indicate a true flare, though the decay ought to be more gradual
  than the incline.
\item \emph{midpoint check}: A stellar flare typically requires a
  monotonic increase then monotonic decrease in brightness.  Ensuring
  that the middle point is higher than the beginning and end points of
  the event is one way to rule out falsely marked events.
\item \emph{second derivative}: Smoothing over the flagged event, is
  the light curve locally concave up or down?  The second derivative
  of the window around the potential flare can capture the shape of a
  light curve in the neighborhood of an event.
\item \emph{skew}: Skewness is a measure of the asymmetry of the event
  brightness---is the flare left-leaning or right-leaning?  Because
  flares are characterized by very quick increases in brightness
  followed by a slow decay, left-leaning events (and therefore those
  with negative skew) are more likely to be true flares.
\item \emph{slope}: Is the brightness of the star generally increasing
  or decreasing at the time of the event?  This metric measures the
  slope of the line formed by connecting the point at the beginning of
  the flare window to the point at the end of the flare window.
  time of the event?
\item \emph{slope ratio}: We also compute the ratio of the light
  curve's slope just before the event begins and the slope just after
  it ends.  We hope to capture more information about the local shape
  of the light curve with this metric.
\end{enumerate}
These data were gathered for potential flaring events that we labelled
by-eye to form a training set.

\subsection{Initial Classification Performance}
\label{sec:class}
We tried a few different classification techniques.  We used Python's
\verb|sklearn| package for our machine learning framework.

\subsubsection{Support Vector Classification}
\label{sec:svc}
For our first attempt we used support vector classification as
packaged in \verb|sklearn.svm.SVC|.  We initially used a radial basis
function (RBF) kernel.
\begin{table}
  \centering
  \begin{tabular}[!htbp]{c|c c c c}
                & precision &recall &f1-score &support \\ \hline
    n           & 0.73      &0.81   &0.77     &57      \\
    y           & 0.74      &0.89   &0.81     &61      \\
    m           & 0.71      &0.31   &0.43     &32      \\ \hline
    avg / total & 0.73      &0.73   &0.71     &150     \\
  \end{tabular}
  \caption{Reconstructing the training set (150 flares total) with RBF kernel.}

  \begin{tabular}[!htbp]{c|c c c c}
                & precision &recall &f1-score &support \\ \hline
    n           & 0.61      &0.88   &0.72     &60      \\
    y           & 0.62      &0.60   &0.61     &65      \\
    m           & 0.33      &0.12   &0.18     &40      \\ \hline
    avg / total & 0.55      &0.59   &0.55     &165     \\
  \end{tabular}
  \caption{Classification report for predicting the testing set
    with RBF kernel.}
\end{table}

We also tried using a linear kernel, but this performed worse.

\subsubsection{Random Forest Classifier}
\label{sec:randfor}
Next we attempted the same task using random forest classification,
as packaged in \verb|sklearn.ensemble|.  While it performed superbly
on the training set, it performed about average on the training set.

\begin{table}
  \centering
  \begin{tabular}[!htbp]{c|c c c c}
                & precision &recall &f1-score &support \\ \hline
    n           & 1.00      &0.92   &0.99     &57      \\
    y           & 0.95      &1.00   &0.98     &61      \\
    m           & 1.00      &0.94   &0.97     &32      \\ \hline
    avg / total & 0.98      &0.98   &0.98     &150     \\
  \end{tabular}
  \caption{Reconstructing the training set (150 flares total) with
    Random Forest classification method.}

  \begin{tabular}[!htbp]{c|c c c c}
                & precision &recall &f1-score &support \\ \hline
    n           & 0.63      &0.77   &0.69     &60      \\
    y           & 0.60      &0.72   &0.66     &65      \\
    m           & 0.29      &0.10   &0.15     &40      \\ \hline
    avg / total & 0.54      &0.59   &0.55     &165     \\
  \end{tabular}
  \caption{Classification report for predicting the testing set
    with the Random Forest classification method.}
\end{table}

\section{Conclusion}
\label{sec:conc}
Ultimately it seems the human element cannot be removed from the
vetting process, but with the aid of machine learning,\ldots
% we can decrease the time spent examining

\end{document}